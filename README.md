# ServingML

## О проекте

**ServingML** — это высокопроизводительный gRPC-сервер для инференса моделей машинного обучения, специализирующийся на русскоязычных BERT-моделях в формате ONNX. Система обеспечивает анализ тональности и распознавание эмоций в текстах с применением динамического батчинга для оптимальной производительности.

### Основные возможности

- **Анализ тональности** (russian-sentiment): классификация текста как нейтральный, позитивный или негативный
- **Распознавание эмоций** (emotion-detection): идентификация 28 различных эмоций в русском тексте
- **Динамический батчинг**: автоматическое формирование батчей запросов для эффективной утилизации ресурсов
- **gRPC API**: высокопроизводительный интерфейс для инференса
- **Конкурентная обработка**: параллельная обработка множественных запросов
- **Ленивая инициализация**: запуск обработчиков батчей по требованию

## API

### gRPC эндпоинт

Сервис предоставляет единственный gRPC метод:

| Сервис | Метод | Описание |
| :--- | :--- | :--- |
| BertService | Predict | Выполняет инференс модели на входном тексте |

### Определение protobuf

```protobuf
service BertService {
  rpc Predict (BertRequest) returns (BertResponse);
}

message BertRequest {
  string text = 1;        // Текст для анализа
  string modelName = 2;   // Имя модели: "russian-sentiment" или "emotion-detection"
}

message BertResponse {
  string result = 1;      // Результат классификации
}
```

## Модели

### russian-sentiment

Модель анализа тональности текста

**Возможные результаты:**
- `neutral` — нейтральная тональность
- `positive` — позитивная тональность
- `negative` — негативная тональность

### emotion-detection

Модель распознавания эмоций в русском тексте

**Возможные результаты (28 эмоций):**
восхищение, веселье, злость, раздражение, неодобрение, отвращение, смущение, страх, благодарность, горе, радость, любовь, нервозность, оптимизм, гордость, осознание, облегчение, раскаяние, печаль, удивление, нейтральное, одобрение, забота, смущение_2, любопытство, желание, разочарование, удовольствие

## Технологии и библиотеки

### Язык программирования
- **Go** (версия 1.24.0)

### Основные библиотеки

| Библиотека | Версия | Назначение |
|------------|--------|------------|
| `google.golang.org/grpc` | v1.72.1 | gRPC сервер и клиент |
| `google.golang.org/protobuf` | v1.36.6 | Serialization Protocol Buffers |
| `github.com/yalue/onnxruntime_go` | v1.19.0 | ONNX Runtime для инференса моделей |
| `github.com/daulet/tokenizers` | v1.20.2 | Токенизация текста |
| `go.uber.org/zap` | v1.27.0 | Структурированное логирование |
| `github.com/ilyakaznacheev/cleanenv` | v1.5.0 | Загрузка и валидация конфигурации |
| `github.com/jackc/pgx/v5` | v5.7.5 | Драйвер PostgreSQL |
| `github.com/google/uuid` | v1.6.0 | Генерация UUID |
| `github.com/joho/godotenv` | v1.5.1 | Загрузка переменных окружения |

### Нативные библиотеки (CGO)
- `libonnxruntime.so` — ONNX Runtime для выполнения моделей
- `libtokenizers` — библиотека токенизации

## Структура проекта

```
ServingML/
├── cmd/                         # Точки входа приложения
│   ├── server/                  # Основной gRPC сервер
│   └── client/                  # Тестовый клиент
├── config/                      # Конфигурационные файлы
│   └── config.yaml             # Конфигурация моделей и сервера
├── docker_libs/                 # Нативные библиотеки для Docker
│   └── libonnxruntime.so
├── gen/proto/                   # Сгенерированный код protobuf
├── internal/                    # Внутренняя бизнес-логика
│   ├── app/                    # Инициализация приложения
│   │   └── grpc/               # Настройка gRPC приложения
│   ├── batcher/                # Логика батчинга запросов
│   ├── client/                 # Реализация gRPC клиента
│   ├── config/                 # Загрузка конфигурации
│   ├── domain/models/          # Доменные модели (запросы/ответы)
│   ├── inference/              # Движок инференса моделей
│   ├── modelWrapper/           # Обёртка над ONNX моделями
│   │   └── data/               # ONNX модели и токенизаторы
│   └── transport/grpc/         # Реализация gRPC сервера
├── pkg/                         # Переиспользуемые пакеты
│   ├── converter/              # Конвертация вероятностей в метки
│   ├── logger/                 # Обёртка над Zap логгером
│   └── modelUtils/             # Операции с тензорами, softmax, батчинг
└── proto/                       # Определения protobuf
    └── model/
        └── model.proto         # Описание BertService
```

## Конфигурация

Конфигурация хранится в файле `config/config.yaml`:

```yaml
grpc_port: 6060          # Порт gRPC сервера
grpc_host: 0.0.0.0       # Хост для прослушивания
batch_timeout: 50        # Таймаут в миллисекундах перед обработкой неполного батча
timeout: 5               # Время простоя в секундах перед остановкой процессора батчей

models:
  - name: "russian-sentiment"
    tokenizer_path: "/path/to/tokenizer.json"
    model_path: "/path/to/model.onnx"
    input_size: 1        # Размерность входа
    output_size: 3       # Количество классов (neutral, positive, negative)
    batch_size: 4        # Максимальный размер батча

  - name: "emotion-detection"
    tokenizer_path: "/path/to/tokenizer.json"
    model_path: "/path/to/model.onnx"
    input_size: 1
    output_size: 28      # Количество классов эмоций
    batch_size: 4
```

### Параметры батчинга

- **batch_size**: Максимальное количество запросов в одном батче (по умолчанию 4)
- **batch_timeout**: Максимальное время ожидания формирования полного батча в миллисекундах (по умолчанию 50ms)
- **timeout**: Время простоя в секундах, после которого процессор батчей автоматически останавливается (по умолчанию 5s)

## Установка и запуск

### Предварительные требования

- **Go** версии 1.24.0 или выше: [Скачать Go](https://go.dev/doc/install)
- **Docker** (опционально для контейнеризации)
- **Git** для клонирования репозитория

### Клонирование репозитория

```bash
git clone https://github.com/yourusername/ServingML.git
cd ServingML
```

### Установка зависимостей

```bash
go mod download
```

### Запуск сервера локально

#### С использованием Go

```bash
# Из корневой директории проекта
go run cmd/server/main.go
```

Сервер запустится на `0.0.0.0:6060`

#### С использованием Docker

```bash
# Сборка Docker образа
docker build -t servingml:latest .

# Запуск контейнера
docker run -p 6060:6060 servingml:latest
```

Сервер будет доступен по адресу `localhost:6060`

## Примеры использования

### Использование тестового клиента

Проект включает тестовый клиент, который демонстрирует работу с gRPC API:

```bash
go run cmd/client/main.go
```

Клиент выполняет:
- 10 горутин, каждая отправляет 100 запросов для модели `russian-sentiment`
- Пауза 10 секунд
- 10 горутин, каждая отправляет 100 запросов для модели `emotion-detection`
- Вывод времени выполнения для каждой модели

### Использование с Go клиентом

```go
package main

import (
    "context"
    "log"
    "time"

    pb "ServingML/gen/proto/model"
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials/insecure"
)

func main() {
    // Подключение к серверу
    conn, err := grpc.Dial("localhost:6060",
        grpc.WithTransportCredentials(insecure.NewCredentials()))
    if err != nil {
        log.Fatalf("Failed to connect: %v", err)
    }
    defer conn.Close()

    client := pb.NewBertServiceClient(conn)

    // Анализ тональности
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    resp, err := client.Predict(ctx, &pb.BertRequest{
        Text:      "Это замечательный день!",
        ModelName: "russian-sentiment",
    })
    if err != nil {
        log.Fatalf("Predict failed: %v", err)
    }

    log.Printf("Result: %s", resp.Result)
}
```

## Как это работает

### Поток обработки запроса

1. **gRPC запрос** поступает на метод `Predict`
2. **Маршрутизация**: запрос направляется в соответствующий `ServiceBatcher` по имени модели
3. **Батчинг**:
   - Запрос добавляется в очередь модели
   - При первом запросе запускается процессор батчей (ленивая инициализация)
   - Батч формируется при достижении `batch_size` или истечении `batch_timeout`
   - Если нет новых запросов в течение `timeout` секунд, процессор останавливается
4. **Инференс**:
   - Токенизация всех текстов в батче
   - Создание входных тензоров (input_ids, token_type_ids, attention_mask)
   - Выполнение ONNX модели
   - Применение softmax к логитам
   - Возврат вероятностей каждому запросу через каналы ответов
5. **Конвертация**: преобразование вероятностей в человекочитаемые метки
6. **Ответ**: отправка результата клиенту через gRPC

### Архитектурные особенности

- **Динамический батчинг**: накопление запросов до размера батча или таймаута
- **Отдельные очереди для каждой модели**: независимая обработка различных моделей
- **Автоматическое отключение**: процессоры батчей останавливаются после периода неактивности
- **Потокобезопасность**: использование mutex-блокировок при инференсе
- **Конкурентность**: несколько горутин обрабатывают различные очереди моделей одновременно
- **Graceful shutdown**: корректная обработка сигналов и освобождение ресурсов

## Производительность

Система оптимизирована для:
- **Высокой пропускной способности**: батчинг увеличивает throughput в 3-4 раза
- **Низкой задержки**: batch_timeout в 50ms обеспечивает быстрый отклик
- **Эффективного использования ресурсов**: GPU/CPU утилизация оптимизирована батчингом
- **Масштабируемости**: поддержка множественных конкурентных запросов

## Разработка

### Генерация protobuf кода

```bash
protoc --go_out=. --go_opt=paths=source_relative \
    --go-grpc_out=. --go-grpc_opt=paths=source_relative \
    proto/model/model.proto
```

### Запуск тестов

```bash
go test ./...
```

### Сборка

```bash
# Сборка сервера
go build -o bin/server cmd/server/main.go

# Сборка клиента
go build -o bin/client cmd/client/main.go
```

## Docker

### Multi-stage сборка

Dockerfile использует двухэтапную сборку:

1. **Builder stage**:
   - Скачивание библиотек токенизации
   - Сборка Go бинарника с CGO

2. **Runtime stage**:
   - Минимальный Debian образ
   - Только runtime зависимости
   - Копирование моделей и конфигурации

### Переменные окружения

- `LD_LIBRARY_PATH=/usr/lib` — путь к разделяемым библиотекам
- `ONNXRUNTIME_SHARED_LIB_PATH=/usr/lib/libonnxruntime.so` — путь к ONNX Runtime